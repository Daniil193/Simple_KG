{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "import json\n",
    "import pymorphy2\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words():\n",
    "    stopwords = []\n",
    "    path_to_file = \"stopwords/Stopwords.txt\"\n",
    "    with open(path_to_file, \"r\", encoding=\"utf-8\") as fl:\n",
    "        for line in fl:\n",
    "            stopwords.append(line.strip(\"\\n\"))\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_form(morph, word):\n",
    "    return morph.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-development",
   "metadata": {},
   "source": [
    "## Load preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma,ner,depparse', use_gpu = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-filename",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = load_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0fc400",
   "metadata": {},
   "source": [
    "## Data was loaded from source:   https://gorod.mos.ru/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('temp.csv', sep=\"$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-rogers",
   "metadata": {},
   "source": [
    "### Clearing & Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lens\"] = df[\"message\"].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df[\"lens\"] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[\"theme_value\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df_filtered[df_filtered[\"theme_value\"]==0]\n",
    "df_1 = df_filtered[df_filtered[\"theme_value\"]==1]\n",
    "df_2 = df_filtered[df_filtered[\"theme_value\"]==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-induction",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = df_0[\"message\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sentences = [sent for corp in full_corpus for sent in sent_tokenize(corp, language=\"russian\")]\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    sentences = [sent for corp in full_corpus for sent in sent_tokenize(corp, language=\"russian\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sents = [i for i in sentences if len(i) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_sents), len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-steps",
   "metadata": {},
   "source": [
    "## Get Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "triplets = []\n",
    "for s in tqdm(long_sents):\n",
    "    doc = nlp(s)\n",
    "    for sent in doc.sentences:\n",
    "            entities = [ent.text for ent in sent.ents]\n",
    "            res_d = dict()\n",
    "            temp_d = dict()\n",
    "            for word in sent.words:\n",
    "                temp_d[word.text] = {\"head\": sent.words[word.head-1].text, \"dep\": word.deprel, \"id\": word.id}\n",
    "            for k in temp_d.keys():\n",
    "                nmod_1 = \"\"\n",
    "                nmod_2 = \"\"\n",
    "                if (temp_d[k][\"dep\"] in [\"nsubj\", \"nsubj:pass\"]) & (k in entities):\n",
    "                    res_d[k] = {\"head\": temp_d[k][\"head\"]}\n",
    "                    \n",
    "                    for k_0 in temp_d.keys():\n",
    "                        if (temp_d[k_0][\"dep\"] in [\"obj\", \"obl\"]) &\\\n",
    "                           (temp_d[k_0][\"head\"] == res_d[k][\"head\"]) &\\\n",
    "                            (temp_d[k_0][\"id\"] > temp_d[res_d[k][\"head\"]][\"id\"]):\n",
    "                            res_d[k][\"obj\"] = k_0\n",
    "                            break\n",
    "                    \n",
    "                    for k_1 in temp_d.keys():\n",
    "                        if (temp_d[k_1][\"head\"] == res_d[k][\"head\"]) & (k_1 == \"не\"):\n",
    "                            res_d[k][\"head\"] = \"не \"+res_d[k][\"head\"]\n",
    "                    \n",
    "                    if \"obj\" in res_d[k].keys():\n",
    "                        for k_4 in temp_d.keys():\n",
    "                            if (temp_d[k_4][\"dep\"] ==\"nmod\") &\\\n",
    "                               (temp_d[k_4][\"head\"] == res_d[k][\"obj\"]):\n",
    "                                nmod_1 = k_4\n",
    "                                break\n",
    "                                \n",
    "                        for k_5 in temp_d.keys():\n",
    "                            if (temp_d[k_5][\"dep\"] ==\"nummod\") &\\\n",
    "                               (temp_d[k_5][\"head\"] == nmod_1):\n",
    "                                nmod_2 = k_5\n",
    "                                break\n",
    "                        res_d[k][\"obj\"] = res_d[k][\"obj\"]+\" \"+nmod_2+\" \"+nmod_1\n",
    "\n",
    "            if len(res_d) > 0:\n",
    "                triplets.append([s, res_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_text = lambda x: \"\".join(i if (i.isdigit()) | (i.isalpha()) | (i in [\" \"]) else \" \" for i in x )\n",
    "\n",
    "clear_triplets = dict()\n",
    "for tr in triplets:\n",
    "    for k in tr[1].keys():\n",
    "        if \"obj\" in tr[1][k].keys():\n",
    "            ## clear_text убрать, если не нужна очистка предложений\n",
    "            clear_triplets[clear_text(tr[0])] =  [k, tr[1][k]['head'], tr[1][k]['obj']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_df = []\n",
    "for k in clear_triplets.keys():\n",
    "    for_df.append([k]+clear_triplets[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-asian",
   "metadata": {},
   "source": [
    "## Create DF for prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triplets = pd.DataFrame(for_df, columns=[\"full_sent\", \"subject\", \"verb\", \"object\"])\n",
    "df_triplets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triplets[\"subj_n_f\"] = df_triplets[\"subject\"].apply(lambda x: norm_form(morph, x))\n",
    "df_triplets[\"obj_n_f\"] = df_triplets[\"object\"].apply(lambda x: norm_form(morph, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triplets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_triplets[(~df_triplets[\"subj_n_f\"].isin(stopwords)) &\\\n",
    "                          (~df_triplets[\"obj_n_f\"].isin(stopwords))].sort_values(by=\"obj_n_f\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-buying",
   "metadata": {},
   "source": [
    "## Split data on chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = list(chunks(df_filtered[\"obj_n_f\"].unique(), 100))\n",
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_num = 0\n",
    "df_for_draw = df_filtered[df_filtered[\"obj_n_f\"].isin(groups[gr_num])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.unique(df_for_draw[[\"subj_n_f\", \"obj_n_f\"]].values.ravel(\"K\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-rhythm",
   "metadata": {},
   "source": [
    "## Get edges & edges info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d_d = df_for_draw.drop_duplicates(subset=[\"subj_n_f\", \"obj_n_f\", \"verb\"])[[\"subj_n_f\", \"obj_n_f\", \"verb\", \"full_sent\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d_d.shape, df_for_draw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = dict()\n",
    "label_dict = dict()\n",
    "for cc, raw in enumerate(df_d_d.values):\n",
    "    info_dict[(raw[0], raw[1])] = {f\"sent_{cc}\": raw[3]}\n",
    "    label_dict[(raw[0], raw[1])] = raw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_num = dict()\n",
    "for c, word in enumerate(nodes):\n",
    "    word_num[word] = c+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-tennessee",
   "metadata": {},
   "source": [
    "## Draw Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import script_for_graph\n",
    "import importlib\n",
    "importlib.reload(script_for_graph)\n",
    "from script_for_graph import header_text, tail_text\n",
    "\n",
    "header_text += \"\"\"\\nvar nodes = new vis.DataSet([\\n\"\"\"\n",
    "for w in nodes:\n",
    "    header_text += \"{\"\n",
    "    header_text += f\"\"\"         id: {word_num[w]}, \n",
    "                                label: \"{w}\"\\n\"\"\"\n",
    "    header_text += \"},\"\n",
    "header_text += \"   ]);\\n\"\n",
    "\n",
    "header_text += \"\"\"var edges = new vis.DataSet([\"\"\"\n",
    "for k in info_dict.keys():\n",
    "    header_text += \"{\"\n",
    "    header_text += f\"\"\"       from: {word_num[k[0]]}, \n",
    "                    to: {word_num[k[1]]}, \n",
    "                    arrows: \"to\",\n",
    "                    label: \"{label_dict[k]}\",\n",
    "                    info: {info_dict[k]}\\n\"\"\"\n",
    "    header_text +=\"},\"\n",
    "header_text += \"   ]);\\n\"\n",
    "\n",
    "full_text = \"\"\n",
    "full_text += header_text\n",
    "full_text += tail_text\n",
    "\n",
    "with open(f\"Graph_for_group_{gr_num}.html\", \"w\", encoding=\"utf-8\") as f: \n",
    "    f.write(full_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
